{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg_ReNhJEHLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8918b9-2207-48f1-f5d7-65410e287200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "assets_url = 'gdrive/My Drive/Colab Notebooks/nlp-2023/assets/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JVfO-RIE_lI",
        "outputId": "0f0d2946-55ef-4875-942e-84e6ebb39e6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stops = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "6TIeIZvco9bk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "trigrams = []\n",
        "trigram_frequences_dict = dict()\n",
        "word_frequences_dict = dict()\n",
        "S = 0\n",
        "\n",
        "text = ''\n",
        "\n",
        "file_no = 0\n",
        "for address, dirs, files in os.walk(assets_url):\n",
        "  for name in files:\n",
        "    file_no += 1\n",
        "    with open(os.path.join(address, name), mode='r') as annotated_document_file:\n",
        "      for sentence in annotated_document_file.read().split('\\n\\n'):\n",
        "        lemms = []\n",
        "        for annotation in sentence.split('\\n'):\n",
        "          word_stem_lem = annotation.split('\\t')\n",
        "          if len(word_stem_lem) == 3:\n",
        "            lemma = word_stem_lem[2]\n",
        "            # Очистить полученные данные от знаков пунктуации. Можно использовать регулярное выражение: [^\\P{P}-]+;\n",
        "            if not re.match('[^\\P{P}-]+', lemma):\n",
        "              lemma = re.sub('[^\\P{P}-]+', '', lemma).lower() # Привести полученные данные к нижнему регистру;\n",
        "              # Очистить полученные данные от стоп слов. Можно использовать nltk.corpus.stopwords;\n",
        "              if not lemma in stops:\n",
        "                lemms.append(lemma)\n",
        "                # fill word frquences dictionary\n",
        "                word_frequences_dict[lemma] = word_frequences_dict.get(lemma, 0) + 1\n",
        "                S += 1\n",
        "        for i in range(len(lemms) - 2):\n",
        "          trigrams.append((lemms[i], lemms[i+1], lemms[i+2]))\n",
        "          # fill trigram frquences dictionary\n",
        "          trigram_frequences_dict[trigrams[-1]] = trigram_frequences_dict.get(trigrams[-1], 0) + 1\n",
        "        text += ' '.join(lemms) + '.\\n'\n",
        "print(len(trigrams))\n",
        "# выполняется 2 часа"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTk_1xpzN7JL",
        "outputId": "411bb6cc-793d-4b12-a4cc-12e0e1ac48cf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2144244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "trigram_mi_scores_dict = dict()\n",
        "\n",
        "# calculate MI\n",
        "for trigram, trigram_freq in list(trigram_frequences_dict.items()):\n",
        "  lemm_1_freq = word_frequences_dict[trigram[0]]\n",
        "  lemm_2_freq = word_frequences_dict[trigram[1]]\n",
        "  lemm_3_freq = word_frequences_dict[trigram[2]]\n",
        "  trigram_mi_scores_dict[trigram] = math.log2(trigram_freq * (S**2) / (lemm_1_freq * lemm_2_freq * lemm_3_freq))"
      ],
      "metadata": {
        "id": "8sWzEfeMDD3J"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top-30 by mine MI\n",
        "sorted_mi_scores = sorted(trigram_mi_scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "print('top 1-30 trigrams by mine MI')\n",
        "for trigram, mi_score in sorted_mi_scores[:30]:\n",
        "  print(f\"{trigram} - MI = {mi_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_wMoWnqmpfW",
        "outputId": "44b025bb-5629-41ed-9131-21bc45f131eb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 1-30 trigrams by mine MI\n",
            "('polly', 'umrigar', 'dilip') - MI = 42.622395696397916\n",
            "('umrigar', 'dilip', 'vengsarkar') - MI = 42.622395696397916\n",
            "('dilip', 'vengsarkar', 'bapu') - MI = 42.622395696397916\n",
            "('vengsarkar', 'bapu', 'nadkarni') - MI = 42.622395696397916\n",
            "('andrival', 'kalle', 'palander') - MI = 42.622395696397916\n",
            "('seyes', 'devil inca', 'rnate') - MI = 42.622395696397916\n",
            "('photomanuel', 'balce', 'ceneta') - MI = 42.622395696397916\n",
            "('battalion', 'sturdyspanish', 'baseliners') - MI = 42.622395696397916\n",
            "('huysegms', 'martijn', 'meerdink') - MI = 42.622395696397916\n",
            "('guayre', 'betancor', 'javi') - MI = 42.622395696397916\n",
            "('betancor', 'javi', 'venta') - MI = 42.622395696397916\n",
            "('holdover', 'ryne', 'sandberg') - MI = 42.622395696397916\n",
            "('salihamidzic', 'torsten', 'frings') - MI = 42.622395696397916\n",
            "('sati', 'pradha', 'mela') - MI = 42.622395696397916\n",
            "('droo', 'nuhs', 'ihl') - MI = 42.622395696397916\n",
            "('pero', 'nueve', 'puntos') - MI = 42.622395696397916\n",
            "('beverlys', 'nicki', 'silveira') - MI = 42.622395696397916\n",
            "('tps', 'turku', 'thefinnish') - MI = 42.622395696397916\n",
            "('receiverdefensive', 'backkick', 'returnerkicker') - MI = 42.622395696397916\n",
            "('busquaert', 'havejoined', 'hansons') - MI = 42.622395696397916\n",
            "('riva', 'dei', 'tessali') - MI = 42.622395696397916\n",
            "('102304', 'so ince', 'nsed') - MI = 42.622395696397916\n",
            "('mausoleum', 'carpe', 'diem') - MI = 42.622395696397916\n",
            "('wltpctpfpanew', 'england4001', '00010563') - MI = 42.622395696397916\n",
            "('jets4001', '0009875', 'buffalo040') - MI = 42.622395696397916\n",
            "('0009875', 'buffalo040', '0005173') - MI = 42.622395696397916\n",
            "('buffalo040', '0005173', 'miami050') - MI = 42.622395696397916\n",
            "('0005173', 'miami050', '0004287 ') - MI = 42.622395696397916\n",
            "('june 11', 'kamila', 'vodichkova') - MI = 42.622395696397916\n",
            "('coober', 'pedy', 'waterhole') - MI = 42.622395696397916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.collocations import *\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "\n",
        "tokens = nltk.word_tokenize(text, 'english', True)\n",
        "print(tokens[:10])\n",
        "\n",
        "text = nltk.Text(tokens)\n",
        "\n",
        "#http://www.nltk.org/_modules/nltk/collocations.html\n",
        "finder_thr = TrigramCollocationFinder.from_words(text)\n",
        "\n",
        "print('top 1-30 trigrams by nltk PMI')\n",
        "thirty_best_trigrams_nltk = finder_thr.nbest(trigram_measures.pmi, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Ov3hvoX-dwgY",
        "outputId": "44a00fea-662d-4ab8-84d2-4559a4f21a14"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-48446c12e78a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrigram_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrigramAssocMeasures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     return [\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/tokenize/destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thirty_best_trigrams_nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "VrPvoGzGdA2E",
        "outputId": "b8a5cce3-5889-42c6-9429-2cbc1b0ef5c4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-ca688f2f24bb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthirty_best_trigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'thirty_best_trigrams' is not defined"
          ]
        }
      ]
    }
  ]
}