{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoROD_vQ6hIT"
      },
      "outputs": [],
      "source": [
        "# https://github.com/Ko4eBHuK/nlp-23-spring/tree/main/tasks/task-03\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-DS1Afq6lCC"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "assets_url = 'gdrive/My Drive/Colab Notebooks/nlp-2023/assets/train/'\n",
        "assets_generated_url = 'gdrive/My Drive/Colab Notebooks/nlp-2023/assets/task-3/'\n",
        "stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO2zh8Z4U-iF"
      },
      "outputs": [],
      "source": [
        "token_freq_dict = dict()\n",
        "file_names_contents_dict = dict()\n",
        "S = 0\n",
        "\n",
        "for address, dirs, files in os.walk(assets_url):\n",
        "  for file_name in files:\n",
        "    file_content = []\n",
        "    with open(os.path.join(address, file_name), mode='r') as annotated_document_file:\n",
        "      for sentence in annotated_document_file.read().split('\\n\\n'):\n",
        "        for annotation in sentence.split('\\n'):\n",
        "          word_stem_lem = annotation.split('\\t')\n",
        "          if len(word_stem_lem) == 3:\n",
        "            token = word_stem_lem[0]\n",
        "            # Очистить полученные данные от знаков пунктуации. Можно использовать регулярное выражение: [^\\P{P}-]+;\n",
        "            if not re.match('[^\\P{P}-]+', token):\n",
        "              token = re.sub('[^\\P{P}-]+', '', token) # Привести полученные данные к нижнему регистру;\n",
        "              # Очистить полученные данные от стоп слов. Можно использовать nltk.corpus.stopwords;\n",
        "              if not token in stops:\n",
        "                # Словарь токенов с их частотами по всем данным\n",
        "                token_freq_dict[token] = token_freq_dict.get(token, 0) + 1\n",
        "                file_content.append(token)\n",
        "                S += 1\n",
        "    file_names_contents_dict[file_name] = file_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnE-XBA9pyjp"
      },
      "outputs": [],
      "source": [
        "# save files-contents dict\n",
        "with open(assets_generated_url + 'file-content-dict.tsv', 'w') as tsvfile:\n",
        "        for key, value in file_names_contents_dict.items():\n",
        "            tsvfile.write(f\"{key}\\t{value}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read file-content-dict, token_freq and term-doc-matrix from files\n",
        "file_names_contents_df = pd.read_table(assets_generated_url + 'file-content-dict.tsv', )\n",
        "tokens_freq_df = pd.read_csv(assets_generated_url + 'token-freq.csv')"
      ],
      "metadata": {
        "id": "DbWAyBL2bCti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_doc_matrix_df = pd.read_csv(assets_generated_url + 'term-doc-matrix.csv', index_col=0)"
      ],
      "metadata": {
        "id": "4IPXVt3qCdBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_names_contents_df"
      ],
      "metadata": {
        "id": "uRUhOSyh-kn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOTLSzSgqjxP"
      },
      "outputs": [],
      "source": [
        "tokens_freq_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "term_doc_matrix_df"
      ],
      "metadata": {
        "id": "ZCkLpbmTbsng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZEc4gViOfal"
      },
      "outputs": [],
      "source": [
        "# 1. Результаты необходимо сохранить во внешние файлы в произвольном формате\n",
        "  # Сохранение словаря частот токенов\n",
        "import csv\n",
        "\n",
        "token_freq_csv_file_url = assets_generated_url + 'token-freq.csv'\n",
        "\n",
        "with open(token_freq_csv_file_url, 'w') as csvfile:\n",
        "        csvfile.write('token,frequency\\n')\n",
        "        for key, value in token_freq_dict.items():\n",
        "            csvfile.write(f\"{key},{value}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZR381EpS72y"
      },
      "outputs": [],
      "source": [
        "top_2k_tokens_by_freq = dict(sorted(dict(zip(tokens_freq_df.token, tokens_freq_df.frequency)).items(), key=lambda x: x[1], reverse=True)[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_50k_docs_with_content = dict(list(dict(zip(file_names_contents_df.file_name, file_names_contents_df.content)).items())[:50000])"
      ],
      "metadata": {
        "id": "20oaKXav6ZSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_50k_docs_with_content"
      ],
      "metadata": {
        "id": "1rMxcRPbCJjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeCH1NgEZCam"
      },
      "outputs": [],
      "source": [
        "# generate term-doc matrix\n",
        "term_doc_matrix = pd.DataFrame(data=0, index=first_50k_docs_with_content.keys(), columns=top_2k_tokens_by_freq.keys())\n",
        "\n",
        "for doc_name, tokens in first_50k_docs_with_content.items():\n",
        "  all_tokens = len(tokens)\n",
        "  for token in tokens:\n",
        "    if token in top_2k_tokens_by_freq.keys():\n",
        "      term_doc_matrix[token][doc_name] += 1\n",
        "\n",
        "# 1. Результаты необходимо сохранить во внешние файлы в произвольном формате\n",
        "  # Сохранение term-doc-matrix\n",
        "term_doc_matrix.to_csv(assets_generated_url + 'term-doc-matrix.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WafoWqUXtRui"
      },
      "outputs": [],
      "source": [
        "# 2. Реализовать один из базовых методов векторизации произвольного текста\n",
        "# Разработать метод, позволяющий преобразовать произвольный текст в вектор значений tf-idf,\n",
        "  # с использованием словаря наиболее частых слов и матрицы \"термин-документ\", полученных ранее (на шаге 1);\n",
        "\n",
        "import math\n",
        "\n",
        "def get_TF_vec_of_doc(doc_text, available_tokens):\n",
        "  doc_tokens = re.findall(r'[^\\s.!?\\-;:]+', doc_text)\n",
        "  TF_vec = pd.DataFrame(data=0, index=[0], columns=available_tokens)\n",
        "  for doc_token in doc_tokens:\n",
        "    if doc_token in available_tokens:\n",
        "      TF_vec[doc_token] += 1\n",
        "  for token in available_tokens:\n",
        "    TF_vec[token] = TF_vec[token]/len(available_tokens)\n",
        "  return TF_vec\n",
        "\n",
        "def getIDF_vec(term_doc_matrix, available_tokens):\n",
        "  IDF_vec = pd.DataFrame(data=0, index=[0], columns=available_tokens.keys())\n",
        "  for token in available_tokens:\n",
        "    for index, row in term_doc_matrix.iterrows():\n",
        "      if row[token] != 0:\n",
        "        IDF_vec[token] += 1\n",
        "  for token in available_tokens:\n",
        "    if IDF_vec[token][0] != 0:\n",
        "      IDF_vec[token] = math.log(term_doc_matrix.shape[0] / IDF_vec[token])\n",
        "  return IDF_vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_TF_vec_of_doc('Bebkiovan shmebra wants new anime. Such  two two tow W W W W W W W W W W W W W W W W Wunicly, only it! Reks shemks gres.', top_2k_tokens_by_freq.keys())"
      ],
      "metadata": {
        "id": "atTyhMZz3at3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_doc_matrix.head()"
      ],
      "metadata": {
        "id": "kC92X8cJlHGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf_dataframe = getIDF_vec(term_doc_matrix_df.head(2000), top_2k_tokens_by_freq)"
      ],
      "metadata": {
        "id": "UB9dqSlg-yyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf_dataframe"
      ],
      "metadata": {
        "id": "hEiA7k-VZwYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShrCKy67vEpq"
      },
      "outputs": [],
      "source": [
        "# 3. Реализовать метод, позволяющий векторизовать произвольный текст с использованием нейронных сетей (предлагается использовать стандартную реализацию модели w2v или glove). \n",
        "# Выбранную модель необходимо обучить на обучающей выборке.\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import common_texts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_lists_in_string_representation = list(first_50k_docs_with_content.values())"
      ],
      "metadata": {
        "id": "VpgLtyJnuctK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for list_in_string_representation in list_of_lists_in_string_representation:\n",
        "  list_in_normal_form = list_in_string_representation.strip('][').split(', ')\n",
        "  clear_list = []\n",
        "  for word in list_in_normal_form:\n",
        "    word_clear = word[1:]\n",
        "    word_clear = word_clear[:-1]\n",
        "    clear_list.append(word_clear)\n",
        "  data.append(clear_list)"
      ],
      "metadata": {
        "id": "IOG4HNcxGBOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "0_92-eY0GeEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(\n",
        "    min_count=10,\n",
        "    vector_size=100,\n",
        "    negative=10,\n",
        "    alpha=0.03,\n",
        "    min_alpha=0.0007,\n",
        "    sample=6e-5,\n",
        "    sg=1)"
      ],
      "metadata": {
        "id": "1XworGTMmUYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.build_vocab(data)"
      ],
      "metadata": {
        "id": "ihSvwU_MmvGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=5, report_delay=1)"
      ],
      "metadata": {
        "id": "yCIvBBx4mwmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.save(assets_generated_url + 'word2vec.model')"
      ],
      "metadata": {
        "id": "91zdUrLDz17a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec.load(assets_generated_url + 'word2vec.model')"
      ],
      "metadata": {
        "id": "B5CBDV5l0DDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.get_vector('walk')"
      ],
      "metadata": {
        "id": "EyhwdeMYLzsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1u2NO6xvNwR"
      },
      "outputs": [],
      "source": [
        "# 4. С использованием библиотечной реализации метода подсчета косинусного расстояния между векторными представлениями текста, \n",
        "# продемонстрировать на примерах, что для семантически близких слов модель генерирует вектора, для которых косинусное расстояние меньше, чем для семантически далеких токенов\n",
        "\n",
        "# изменяется от 0 до 1 и чем больше - тем лсова более похожи\n",
        "from scipy import spatial\n",
        "def cosine_lib(a, b):\n",
        "    return 1 - spatial.distance.cosine(a, b)\n",
        "\n",
        "def cosine_similarity_of_vectors(vec_1, vec_2):\n",
        "  min_vec_size = len(vec_1)\n",
        "  if min_vec_size > len(vec_2):\n",
        "    min_vec_size = len(vec_2)\n",
        "  dot_product_of_vectors = 0\n",
        "  vec_1_magnitude = 0\n",
        "  vec_2_magnitude = 0\n",
        "  for vec_param_index in range(0,min_vec_size):\n",
        "    dot_product_of_vectors += vec_1[vec_param_index] * vec_2[vec_param_index]\n",
        "    vec_1_magnitude += vec_1[vec_param_index] * vec_1[vec_param_index]\n",
        "    vec_2_magnitude += vec_2[vec_param_index] * vec_2[vec_param_index]\n",
        "  vec_1_magnitude = math.sqrt(vec_1_magnitude)\n",
        "  vec_2_magnitude = math.sqrt(vec_2_magnitude)\n",
        "\n",
        "  return dot_product_of_vectors / (vec_1_magnitude * vec_2_magnitude)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_of_examples = {\n",
        "\t'step': [['trip', 'walk'], ['count', 'destination'], ['injured', 'center']],\n",
        "\t'coach': [['father', 'player'], ['postgame', 'scoreboard'], ['engagement']],\n",
        "\t'photo': [['picture', 'image', 'painting'], ['file', 'art'], ['postgame', 'wet']],\n",
        "\t'ocean': [['lake', 'water', 'river'], ['fish', 'boat'], ['snap']]\n",
        "}\n",
        "\n",
        "for example_word, tests in set_of_examples.items():\n",
        "  print(f\"analyzing word - {example_word}\")\n",
        "  exmaple_word_vec = w2v_model.wv.get_vector(example_word)\n",
        "  print(\"\\tsame meaning words scores:\")\n",
        "  for same_meaning_word in tests[0]:\n",
        "    same_meaning_word_vec = w2v_model.wv.get_vector(same_meaning_word)\n",
        "    my_cos_dist = cosine_similarity_of_vectors(exmaple_word_vec, same_meaning_word_vec)\n",
        "    lib_cos_dist = cosine_lib(exmaple_word_vec, same_meaning_word_vec)\n",
        "    print(f\"\\t\\tdistance for {same_meaning_word}\")\n",
        "    print(f\"\\t\\t\\t\\tmy  = {my_cos_dist}\")\n",
        "    print(f\"\\t\\t\\t\\tlib = {lib_cos_dist}\")\n",
        "  print(\"\\tsame scope words scores:\")\n",
        "  for same_scope_word in tests[1]:\n",
        "    same_scope_word_vec = w2v_model.wv.get_vector(same_scope_word)\n",
        "    my_cos_dist = cosine_similarity_of_vectors(exmaple_word_vec, same_scope_word_vec)\n",
        "    lib_cos_dist = cosine_lib(exmaple_word_vec, same_scope_word_vec)\n",
        "    print(f\"\\t\\tdistance for {same_scope_word}\")\n",
        "    print(f\"\\t\\t\\t\\tmy  = {my_cos_dist}\")\n",
        "    print(f\"\\t\\t\\t\\tlib = {lib_cos_dist}\")\n",
        "  print(\"\\tdifferent scope words scores:\")\n",
        "  for diff_scope_word in tests[2]:\n",
        "    diff_scope_word_vec = w2v_model.wv.get_vector(diff_scope_word)\n",
        "    my_cos_dist = cosine_similarity_of_vectors(exmaple_word_vec, diff_scope_word_vec)\n",
        "    lib_cos_dist = cosine_lib(exmaple_word_vec, diff_scope_word_vec)\n",
        "    print(f\"\\t\\tdistance for {diff_scope_word}\")\n",
        "    print(f\"\\t\\t\\t\\tmy  = {my_cos_dist}\")\n",
        "    print(f\"\\t\\t\\t\\tlib = {lib_cos_dist}\")"
      ],
      "metadata": {
        "id": "-fwTUogNJC-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ5jRwldvq74"
      },
      "outputs": [],
      "source": [
        "# 5. Применить какой-либо метод сокращения размерностей полученных одним из базовых способов векторизации, выбранным ранее (см. пункт 2), векторов\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "vectors = []\n",
        "df_train = pd.read_csv('gdrive/My Drive/Colab Notebooks/nlp-2023/train.csv')\n",
        "\n",
        "for index, row in df_train.head(350).iterrows():\n",
        "  text = row['title'] + '. ' + row['text']\n",
        "  df_vector = get_TF_vec_of_doc(text, top_2k_tokens_by_freq.keys())\n",
        "  vector_as_list = df_vector.loc[0, :].values.flatten().tolist()\n",
        "  vectors.append(vector_as_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=100)\n",
        "pca_model = pca.fit(vectors)"
      ],
      "metadata": {
        "id": "ilAZaPbCa3ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. С использованием разработанного метода подсчета косинусного расстояния сравнить эффективность метода векторизации с использованием \n",
        "#  нейронных сетей и эффективность базовых методов векторизации с последующим сокращением размерности.\n",
        "# Сформулировать вывод о том, применение какого способа позволяет получить лучшие результаты на выбранном датасете.\n",
        "\n",
        "pca_ini_vector = pca_model.transform(get_TF_vec_of_doc('photo', top_2k_tokens_by_freq.keys()))[0]\n",
        "pca_same_vector = pca_model.transform(get_TF_vec_of_doc('image', top_2k_tokens_by_freq.keys()))[0]\n",
        "pca_diff_vector = pca_model.transform(get_TF_vec_of_doc('water', top_2k_tokens_by_freq.keys()))[0]\n",
        "\n",
        "w2v_ini_vector = w2v_model.wv.get_vector('photo')\n",
        "w2v_same_vector = w2v_model.wv.get_vector('image')\n",
        "w2v_diff_vector = w2v_model.wv.get_vector('water')"
      ],
      "metadata": {
        "id": "Vo8u5CC9j4wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_cos_dist_pca_same = cosine_similarity_of_vectors(pca_ini_vector, pca_same_vector)\n",
        "my_cos_dist_pca_diff = cosine_similarity_of_vectors(pca_ini_vector, pca_diff_vector)\n",
        "\n",
        "lib_cos_dist_pca_same = cosine_lib(pca_ini_vector, pca_same_vector)\n",
        "lib_cos_dist_pca_diff = cosine_lib(pca_ini_vector, pca_diff_vector)\n",
        "\n",
        "\n",
        "my_cos_dist_w2v_same = cosine_similarity_of_vectors(w2v_ini_vector, w2v_same_vector)\n",
        "my_cos_dist_w2v_diff = cosine_similarity_of_vectors(w2v_ini_vector, w2v_diff_vector)\n",
        "\n",
        "lib_cos_dist_w2v_same = cosine_lib(w2v_ini_vector, w2v_same_vector)\n",
        "lib_cos_dist_w2v_diff = cosine_lib(w2v_ini_vector, w2v_diff_vector)"
      ],
      "metadata": {
        "id": "HK1XkmHKpGMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('PCA')\n",
        "print('\\tMY cosine')\n",
        "print(f\"\\t\\tsame={my_cos_dist_pca_same}\\t\\tdiff={my_cos_dist_pca_diff}\")\n",
        "print('\\tLIB cosine')\n",
        "print(f\"\\t\\tsame={lib_cos_dist_pca_same}\\t\\tdiff={lib_cos_dist_pca_diff}\")\n",
        "\n",
        "print()\n",
        "\n",
        "print('W2V')\n",
        "print('\\tMY cosine')\n",
        "print(f\"\\t\\tsame={my_cos_dist_w2v_same}\\t\\tdiff={my_cos_dist_w2v_diff}\")\n",
        "print('\\tLIB cosine')\n",
        "print(f\"\\t\\tsame={lib_cos_dist_w2v_same}\\t\\tdiff={lib_cos_dist_w2v_diff}\")"
      ],
      "metadata": {
        "id": "FBP-tMGDp4e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evBXDL_XwDLn"
      },
      "outputs": [],
      "source": [
        "# 7. Реализовать метод, осуществляющий векторизацию произвольного текста по следующему алгоритму: https://github.com/Ko4eBHuK/nlp-23-spring/tree/main/tasks/task-03\n",
        "def vectorize_text(text):\n",
        "  text_vector = [0] * 100\n",
        "  total_tokens = 0\n",
        "  sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s(?![a-z])', text)\n",
        "  for sentence in sentences:\n",
        "    tokens = re.findall(r'[^\\s.!?\\-;:]+', sentence)\n",
        "    for token in tokens:\n",
        "      if (not token in stops) and (token in w2v_model.wv.key_to_index):\n",
        "        total_tokens += 1\n",
        "        token_vector = w2v_model.wv.get_vector(token)\n",
        "        text_vector = [sum(x) for x in zip(text_vector, token_vector)]\n",
        "  return [x / total_tokens for x in text_vector]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_text('Michael Phelps won the gold medal Wireless in the 400 individual medley and set a world record in a time of 4 minutes 8.26 seconds.')"
      ],
      "metadata": {
        "id": "LoGlxsnZxj6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npBN4jT3wV2N"
      },
      "outputs": [],
      "source": [
        "# 8. Выполнить векторизацию тестовой выборки с использованием метода, реализованного на предыдущем шаге.\n",
        "test_url = 'gdrive/My Drive/Colab Notebooks/nlp-2023/test.csv'\n",
        "df_test = pd.read_csv(test_url)\n",
        "\n",
        "with open(f\"gdrive/My Drive/Colab Notebooks/nlp-2023/assets/task-3/test-embeddings.tsv\", 'w') as writefile:\n",
        "  for index, row in df_test.iterrows():\n",
        "    text = row['title'] + '. ' + row['text']\n",
        "    doc_vector = vectorize_text(text)\n",
        "    row_to_write = f\"{index}\"\n",
        "    for embeding_component in doc_vector:\n",
        "      row_to_write += f\"\\t{embeding_component}\"\n",
        "    writefile.write(row_to_write + '\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}