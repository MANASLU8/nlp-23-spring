{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLQrrZffal-V"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "embedings_url = 'gdrive/My Drive/Colab Notebooks/nlp-2023/assets/task-3/test-embeddings.tsv'\n",
        "test_df_url= 'gdrive/My Drive/Colab Notebooks/nlp-2023/test.csv'"
      ],
      "metadata": {
        "id": "CLEEPvnEcLaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# считать вектора и запустить классификатор SVM"
      ],
      "metadata": {
        "id": "H-WA9N1gbFvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(test_df_url)\n",
        "y_dataset = list(test_df['class'])\n",
        "\n",
        "y_train = y_dataset[:7500]\n",
        "y_test = y_dataset[7500:]"
      ],
      "metadata": {
        "id": "SwKaOTNQ1PDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. С использованием полученных в результате выполнения четвертой лабораторной работы векторных представлений документов осуществить эксперименты с моделью SVM для многоклассовой классификации.\n",
        "columns = [f\"emb_{x}\" for x in range(0,100)]\n",
        "embedings = pd.read_table(embedings_url, names=columns)"
      ],
      "metadata": {
        "id": "djTUN8DjbLAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_dataset = []\n",
        "for value in embedings.values:\n",
        "  x_dataset.append(value.flatten().tolist())\n",
        "\n",
        "x_train = x_dataset[:7500]\n",
        "x_test = x_dataset[7500:]"
      ],
      "metadata": {
        "id": "i5zxkRgmcTmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics calculating\n",
        "def confusion_matrix(actuals, predicted):\n",
        "  df = pd.crosstab(actuals, predicted, rownames=['Actual'], colnames=['Predicted'])\n",
        "  for index, row in df.iterrows():\n",
        "    if index not in df.columns:\n",
        "      df[index] = 0\n",
        "  return df\n",
        "\n",
        "def accuracy(conf_matrix_df):\n",
        "  sum_all = conf_matrix_df.values.sum()\n",
        "  pos = 0\n",
        "  for index, row in conf_matrix_df.iterrows():\n",
        "    try:\n",
        "      pos += conf_matrix_df[index].loc[index]\n",
        "    except:\n",
        "      pass\n",
        "  return round(pos/sum_all, 5)\n",
        "\n",
        "def precision(conf_matrix_df):\n",
        "    return round((np.diag(conf_matrix_df) / np.sum(conf_matrix_df, axis=0)).mean(axis=0), 5)\n",
        "\n",
        "def recall(conf_matrix_df):\n",
        "    return round((np.diag(conf_matrix_df) / np.sum(conf_matrix_df, axis=1)).mean(axis=0), 5)\n",
        "\n",
        "def f1(precision, recall):\n",
        "    return round((2 * precision * recall / (precision + recall)), 5)"
      ],
      "metadata": {
        "id": "sC5pfHey-Da1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from time import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "scaler = StandardScaler() # для нормализации фич\n",
        "X_train = scaler.fit_transform(x_train)\n",
        "X_test = scaler.transform(x_test)\n",
        "\n",
        "def model_experiment(model_name, model):\n",
        "  time_start = time()\n",
        "  model.fit(X_train, y_train)\n",
        "  time_end = time()\n",
        "\n",
        "  actual_classes = y_test\n",
        "  predicted_classes = []\n",
        "  for test in X_test:\n",
        "    predicted_classes.append(model.predict([test])[0])\n",
        "  \n",
        "  conf_matrix = confusion_matrix(actual_classes, predicted_classes)\n",
        "  model_accuracy = accuracy(conf_matrix)\n",
        "  model_precision = precision(conf_matrix)\n",
        "  model_recall = recall(conf_matrix)\n",
        "  model_f1 = f1(model_precision, model_recall)\n",
        "  print(f\"\\t{model_name} metrix:\")\n",
        "  print(f\"\\t\\t train time:{time_end - time_start}\")\n",
        "  print(f\"\\t\\t conf_matrix:\\n{conf_matrix.to_string()}\")\n",
        "  print(f\"\\t\\t accuracy:{model_accuracy}\") # Accuracy represents the number of correctly classified data instances over the total number of data instances.\n",
        "  print(f\"\\t\\t precision:{model_precision}\") # Precision should ideally be 1. Точность TP/(TP+FP)\n",
        "  print(f\"\\t\\t recall:{model_recall}\") # Recall should ideally be 1 (high) for a good classifier. Отклик TP/(TP+FN)\n",
        "  print(f\"\\t\\t f1:{model_f1}\") # F1 score is the harmonic mean of precision and recall and is a better measure than accuracy\n",
        "  print(f\"--------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "8sfo6ZwwT1TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from time import time\n",
        "\n",
        "scaler = StandardScaler() # для нормализации фич\n",
        "X_train = scaler.fit_transform(x_train)\n",
        "X_test = scaler.transform(x_test)\n",
        "\n",
        "for epochs_count in range(100, 1001, 100):\n",
        "  print(f\"Epochs count - {epochs_count}\")\n",
        "  \n",
        "  models = {\n",
        "    'linear': make_pipeline(StandardScaler(), svm.SVC(kernel='linear', degree=2, max_iter=epochs_count)),\n",
        "    'poly2': make_pipeline(StandardScaler(), svm.SVC(kernel='poly', degree=2, max_iter=epochs_count)),\n",
        "    'poly3': make_pipeline(StandardScaler(), svm.SVC(kernel='poly', degree=3, max_iter=epochs_count)),\n",
        "    'poly4': make_pipeline(StandardScaler(), svm.SVC(kernel='poly', degree=4, max_iter=epochs_count)),\n",
        "    'rbf': make_pipeline(StandardScaler(), svm.SVC(kernel='rbf', max_iter=epochs_count)),\n",
        "    'sigmoid': make_pipeline(StandardScaler(), svm.SVC(kernel='sigmoid', max_iter=epochs_count)),\n",
        "  }\n",
        "  \n",
        "  for model_name, model in models.items():\n",
        "    model_experiment(model_name, model)"
      ],
      "metadata": {
        "id": "wtK13pAIfFnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. После выполнения экспериментов на основании полученных значений метрик сформулировать вывод о том, какое количество итераций является оптимальным, \n",
        "  # а также какая модель (или kernel function) показала лучшие результаты (в том случае, если в предыдущем пункте выполнялись эксперименты с несколькими моделями или kernel functions);\n",
        "\n",
        "# По полученным метрикам rbf в качестве kernel function показала лучшие результаты. \n",
        "  # Моя гипотеза такова, что оптимальным количеством эпох будет 800, так как метрики начинают выходить на свои плато"
      ],
      "metadata": {
        "id": "JDyy0oTGbMRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. В соответствии с одним из следующих преобразований над векторными представлениями документов осуществить еще одну серию экспериментов \n",
        "  # над выбранной на предыдущем шаге моделью с использованием найденного оптимального значения для количества итераций (варианты приведены в порядке увеличения сложности реализации):\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "jHorqYUTbZfG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}